{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM FOREST"
      ],
      "metadata": {
        "id": "5i-wmVkOhkrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur1CwDu4ryGo",
        "outputId": "79188e5d-2f00-42f0-c75f-e46bd277ddf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset from CSV\n",
        "def load_dataset(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            text = row['Message']\n",
        "            label = 1 if row['Category'].lower() == 'spam' else 0  # Spam = 1, Ham = 0\n",
        "            data.append((text.lower().split(), label))\n",
        "    return data\n",
        "\n",
        "# Convert text to feature vector\n",
        "def preprocess(data):\n",
        "    vocab = set()\n",
        "    processed_data = []\n",
        "    for tokens, label in data:\n",
        "        vocab.update(tokens)\n",
        "        processed_data.append((tokens, label))\n",
        "    return processed_data, list(vocab)\n",
        "\n",
        "def text_to_features(tokens, vocab):\n",
        "    token_set = set(tokens)\n",
        "    return [1 if word in token_set else 0 for word in vocab]\n",
        "\n",
        "def split_data(data, split_ratio=0.8):\n",
        "    random.shuffle(data)\n",
        "    split_index = int(len(data) * split_ratio)\n",
        "    return data[:split_index], data[split_index:]\n",
        "\n",
        "# Decision Tree\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, data, features):\n",
        "        self.tree = self._build_tree(data, features, depth=0)\n",
        "\n",
        "    def _build_tree(self, data, features, depth):\n",
        "        labels = [label for _, label in data]\n",
        "        if len(set(labels)) == 1 or depth == self.max_depth:\n",
        "            return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "        best_feature = random.choice(features)\n",
        "        left_split = [(x, y) for x, y in data if x[best_feature] == 1]\n",
        "        right_split = [(x, y) for x, y in data if x[best_feature] == 0]\n",
        "\n",
        "        if not left_split or not right_split:\n",
        "            return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "        return {\n",
        "            \"feature\": best_feature,\n",
        "            \"left\": self._build_tree(left_split, features, depth + 1),\n",
        "            \"right\": self._build_tree(right_split, features, depth + 1),\n",
        "        }\n",
        "\n",
        "    def predict(self, x):\n",
        "        node = self.tree\n",
        "        while isinstance(node, dict):\n",
        "            if x[node[\"feature\"]] == 1:\n",
        "                node = node[\"left\"]\n",
        "            else:\n",
        "                node = node[\"right\"]\n",
        "        return node\n",
        "\n",
        "# Random Forest\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees, max_depth):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, data):\n",
        "        for _ in range(self.n_trees):\n",
        "            bootstrap_sample = [random.choice(data) for _ in range(len(data))]\n",
        "            features = random.sample(range(len(data[0][0])), int(math.sqrt(len(data[0][0]))))\n",
        "            tree = DecisionTree(max_depth=self.max_depth)\n",
        "            tree.fit(bootstrap_sample, features)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, x):\n",
        "        votes = [tree.predict(x) for tree in self.trees]\n",
        "        return Counter(votes).most_common(1)[0][0]\n",
        "\n",
        "# Load and preprocess data\n",
        "file_path = \"/content/mail_data.csv\"\n",
        "raw_data = load_dataset(file_path)\n",
        "processed_data, vocab = preprocess(raw_data)\n",
        "features_data = [(text_to_features(tokens, vocab), label) for tokens, label in processed_data]\n",
        "\n",
        "# Split into training and testing\n",
        "train_data, test_data = split_data(features_data)\n",
        "\n",
        "# Train Random Forest\n",
        "forest = RandomForest(n_trees=10, max_depth=3)\n",
        "forest.fit(train_data)\n",
        "\n",
        "# Evaluate on test data\n",
        "correct = 0\n",
        "for x, y in test_data:\n",
        "    if forest.predict(x) == y:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / len(test_data)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "gy4nKchrhg-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z5QWE4C4hvrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-importing required libraries and reloading the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Reloading the dataset\n",
        "file_path = '/content/mail_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Split text into words (tokenization)\n",
        "    return text.split()\n",
        "\n",
        "data['Processed_Message'] = data['Message'].apply(preprocess_text)\n",
        "\n",
        "# Step 2: Encode labels\n",
        "data['Category'] = data['Category'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Step 3: Create a bag-of-words representation\n",
        "# Get a unique vocabulary from all messages\n",
        "vocabulary = list(set(word for message in data['Processed_Message'] for word in message))\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Create a word-to-index mapping\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "# Convert messages to a bag-of-words representation\n",
        "def message_to_vector(message):\n",
        "    vector = np.zeros(vocab_size)\n",
        "    for word in message:\n",
        "        if word in word_to_index:\n",
        "            vector[word_to_index[word]] += 1\n",
        "    return vector\n",
        "\n",
        "# Apply to all messages\n",
        "X = np.array(data['Processed_Message'].apply(message_to_vector).tolist())\n",
        "y = data['Category'].values\n",
        "\n",
        "# Display dimensions of X and y\n",
        "X.shape, y.shape\n",
        "\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Initialize weights and bias\n",
        "weights = np.zeros(X_train.shape[1])\n",
        "bias = 0\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Logistic regression training\n",
        "for i in range(num_iterations):\n",
        "    # Compute predictions\n",
        "    linear_model = np.dot(X_train, weights) + bias\n",
        "    predictions = sigmoid(linear_model)\n",
        "\n",
        "    # Compute gradients\n",
        "    error = predictions - y_train\n",
        "    dw = np.dot(X_train.T, error) / len(y_train)\n",
        "    db = np.sum(error) / len(y_train)\n",
        "\n",
        "    # Update weights and bias\n",
        "    weights -= learning_rate * dw\n",
        "    bias -= learning_rate * db\n",
        "\n",
        "    # Optionally, calculate and print loss every 100 iterations\n",
        "    if i % 100 == 0:\n",
        "        loss = -np.mean(y_train * np.log(predictions) + (1 - y_train) * np.log(1 - predictions))\n",
        "        print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "linear_model_test = np.dot(X_test, weights) + bias\n",
        "test_predictions = sigmoid(linear_model_test)\n",
        "test_predictions = [1 if pred > 0.5 else 0 for pred in test_predictions]\n",
        "\n",
        "# Step 7: Evaluate accuracy\n",
        "accuracy = np.mean(test_predictions == y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHAO7bRWfuWV",
        "outputId": "aea0ea66-9d8a-454e-f322-d40929a0a4e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 0.6931471805599453\n",
            "Iteration 100, Loss: 0.5243828056531046\n",
            "Iteration 200, Loss: 0.45309680502872984\n",
            "Iteration 300, Loss: 0.4123740933931833\n",
            "Iteration 400, Loss: 0.38423801769637306\n",
            "Iteration 500, Loss: 0.36243978814891836\n",
            "Iteration 600, Loss: 0.34439024208107427\n",
            "Iteration 700, Loss: 0.3288658163361045\n",
            "Iteration 800, Loss: 0.31521682437359166\n",
            "Iteration 900, Loss: 0.3030565574937204\n",
            "Test Accuracy: 0.8771300448430494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAIVE BAYE'S"
      ],
      "metadata": {
        "id": "UNoq0QRrhyKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "file_path = '/content/mail_data.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Split text into words\n",
        "    return text.split()\n",
        "\n",
        "# Apply preprocessing\n",
        "data['Processed_Message'] = data['Message'].apply(preprocess_text)\n",
        "data['Category'] = data['Category'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Step 3: Separate spam and ham messages\n",
        "spam_messages = data[data['Category'] == 1]['Processed_Message']\n",
        "ham_messages = data[data['Category'] == 0]['Processed_Message']\n",
        "\n",
        "# Step 4: Calculate word probabilities for each class\n",
        "spam_word_counts = defaultdict(int)\n",
        "ham_word_counts = defaultdict(int)\n",
        "spam_total_words = 0\n",
        "ham_total_words = 0\n",
        "\n",
        "# Count word frequencies in spam and ham messages\n",
        "for message in spam_messages:\n",
        "    for word in message:\n",
        "        spam_word_counts[word] += 1\n",
        "        spam_total_words += 1\n",
        "\n",
        "for message in ham_messages:\n",
        "    for word in message:\n",
        "        ham_word_counts[word] += 1\n",
        "        ham_total_words += 1\n",
        "\n",
        "# Calculate class probabilities\n",
        "p_spam = len(spam_messages) / len(data)\n",
        "p_ham = len(ham_messages) / len(data)\n",
        "\n",
        "# Vocabulary size (for Laplace smoothing)\n",
        "vocabulary = set(word for message in data['Processed_Message'] for word in message)\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Step 5: Define a function to calculate likelihoods\n",
        "def calculate_likelihood(message, class_word_counts, class_total_words, class_prob):\n",
        "    likelihood = np.log(class_prob)  # Start with log of prior probability\n",
        "    for word in message:\n",
        "        # Apply Laplace smoothing\n",
        "        word_prob = (class_word_counts[word] + 1) / (class_total_words + vocab_size)\n",
        "        likelihood += np.log(word_prob)\n",
        "    return likelihood\n",
        "\n",
        "# Step 6: Predict function\n",
        "def predict(message):\n",
        "    # Preprocess message\n",
        "    message = preprocess_text(message)\n",
        "    # Calculate likelihoods for both classes\n",
        "    spam_likelihood = calculate_likelihood(message, spam_word_counts, spam_total_words, p_spam)\n",
        "    ham_likelihood = calculate_likelihood(message, ham_word_counts, ham_total_words, p_ham)\n",
        "    # Return the class with the higher likelihood\n",
        "    return 1 if spam_likelihood > ham_likelihood else 0\n",
        "\n",
        "# Step 7: Evaluate accuracy\n",
        "data['Prediction'] = data['Message'].apply(predict)\n",
        "accuracy = np.mean(data['Prediction'] == data['Category'])\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jv9AA2Fh1ZC",
        "outputId": "7feb2047-04e0-4333-f763-295ae760b95d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9890524048815507\n"
          ]
        }
      ]
    }
  ]
}